<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->
    <style type="text/css">
        @font-face {
            font-family: 'Avenir Book';
            src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
        }
    body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
	<title>OvarNet: Towards Open-vocabulary Object Attribute Recognition</title>
</head>

<body>
<br>
<span style="font-size:36px">
    <div style="text-align: center;">
        OvarNet: Towards Open-vocabulary Object Attribute Recognition
    </div>
</span>
<br>
<br>
<br>
<table align="center" width="800px">
    <tr>
        <td align="center" width="180px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://kyanchen.github.io/">Keyan Chen</a><sup>&star; 1</sup></span>
            </div>
        </td>
        <td align="center" width="180px">
            <div style="text-align: center;">
                <span style="font-size:16px">Xiaolong Jiang<sup>&star; 2</sup></span>
            </div>
        </td>

        <td align="center" width="100px">
            <div style="text-align: center;">
                <span style="font-size:16px">Yao Hu<sup>2</sup></span>
            </div>
        </td>

        <td align="center" width="100px">
            <div style="text-align: center;">
                <span style="font-size:16px">Xu Tang<sup>2</sup></span>
            </div>
        </td>

        <td align="center" width="100px">
            <div style="text-align: center;">
                <span style="font-size:16px">Yan Gao<sup>2</sup></span>
            </div>
        </td>

        <td align="center" width="140px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://github.com/WindVChen">Jianqi Chen</a><sup>1</sup></span>
            </div>
        </td>

        <td align="center" width="140px">
            <div style="text-align: center;">
                <span style="font-size:16px">
                    <a href="https://weidixie.github.io/">Weidi Xie</a>
<!--                    <sup><img class="round" style="width:20px" src="./resources/corresponding_fig.png">3</sup>-->
                    <sup>&#9993 3</sup>
                </span>
            </div>
        </td>
    </tr>
</table>

<br>
	
<table align="center" width="700px">
    <tbody>
    <tr>
        <td align="center" width="300px">
            <center>
                <span style="font-size:16px">Beihang University<sup>1</sup></span>
            </center>
        </td>

        <td align="center" width="300px">
            <center>
                <span style="font-size:16px">Xiaohongshu Inc<sup>2</sup></span>
            </center>
        </td>

        <td align="center" width="300px">
            <center>
                <span style="font-size:16px">Shanghai Jiao Tong University<sup>3</sup></span>
            </center>
        </td>
    </tr>
    </tbody>
</table>


<table align="center" width="700px">
    <tbody>
    <tr>
        <td align="center" width="300px">
            <center>
                <span style="font-size:16px"><sup>&star;</sup>equal contribution</span>
            </center>
        </td>

        <td align="center" width="300px">
            <center>
                <span style="font-size:16px"><sup>&#9993</sup>corresponding author</span>
            </center>
        </td>
    </tr>
    </tbody>
</table>

<table align="center" width="700px">
    <tbody>
    <tr>
        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Code
                    <a href="https://github.com/KyanChen/OvarNet">[GitHub]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Paper
                    <a href="https://arxiv.org/pdf/0">[arXiv]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <center>
                <span style="font-size:20px">
                    Cite <a href="resources/cite.txt"> [BibTeX]</a>
                </span>
            </center>
        </td>
    </tr>
    </tbody>
</table>
<br>
<hr>

<div style="text-align: center;">
    <h2>Teaser</h2>
</div>


<p style="text-align:justify; text-justify:inter-ideograph;">
<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/ovar.png" width="800px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
            The first row depicts the object detection and attribute
            classification tasks in a close-set setting, <i>i.e.</i>,
            training and validating on the same vocabulary set. The below
            line illustrates results of the proposed OvarNet that simultaneously
            localize, categorize, and characterize arbitrary objects in an
            open-vocabulary scenario. We only show one object per image for
            clearly visualization. <span style="color: red; "><b>Red</b></span> denotes
            the base category/attribute <i>i.e.</i>,  seen in the training set, while
            <span style="color: #1230F5; "><b>Blue</b></span> represents the novel
            category/attribute unseen in the training set.
            </p>
        </td>
    </tr>
</table>


<br>
<hr>
<div style="text-align: center;">
    <h2>Abstract</h2>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
            In this paper, we consider the problem of simultaneously localising the objects in an image and inferring
            their categories and attributes, even if they are unseen in the training set, resembling an open-vocabulary
            scenario. To achieve this goal, we make the following four contributions:
            (i) we propose a two-stage approach for open-vocabulary detection and attribute
            classification, termed as CLIP-Attr. The candidate objects are first proposed
            with an offline RPN and each proposal is classified for semantic category and attributes;
            (ii) to steer the pre-trained CLIP to better align the visual representation with attributes,
            we combine the available datasets and train with a federated strategy, in addition,
            we further investigate the efficacy of leveraging freely available online image-caption
            pairs under a novel weakly supervised term;
            (iii) in pursuit of efficiency, we distill the knowledge of CLIP-Attr into a one-stage model, OvarNet,
            that performs class-agnostic object proposals and open-vocabulary classification on semantic category
            and attributes with classifiers generated from a text encoder;
            Finally, (iv) to evaluate our proposed OvarNet,
            we conduct extensive experiments on VAW and MS-COCO datasets.
            OvarNet demonstrates superior performance over existing state-of-the-art
            approaches and shows a strong generalization ability to novel attributes and
            categories unseen during the training. We will publicly release the
            code upon acceptance.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Architecture</h2>
</div>

<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/reverse_models.png" width="800px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <br>
                An overview of the proposed method. <b>Left:</b> the two-step training procedure
                for finetuning the pretrained CLIP to get CLIP-Attr that better
                aligns the regional visual feature to attributes.
                <b>Step-I:</b> naive federate training by base attribute annotations.
                <b>Step-II:</b> training by image-caption pairs.
                We first conduct RPN on the whole image to get box-level crops,
                and parse the caption to get noun phrases, categories, and attributes,
                and then match these fine-grained concepts for weakly supervised training.
                <b>Right:</b> the proposed one-stage framework OvarNet.
                We inherit the CLIP-Attr for open-vocabulary object attribute recognition.
                Regional visual feature is learned from attentional pooling of proposals;
                while attribute concept embedding is extracted from the text encoder.
                Solid lines declare the standard federated training regime.
                Dashed lines denote training by knowledge distillation with CLIP-Attr.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Quantitative Results</h2>
</div>

<table>
    <tr>
        <td>
            <p>
                <b>
                    R1: Benchmark on COCO and VAW Datasets
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                In the Tab., we compare the OvarNet to other attribute prediction methods
                and open-vocabulary object detectors on the VAW test set
                and COCO validation set. Because there is no open-vocabulary
                attribute prediction method developed on the VAW dataset,
                we report two methods with strong performance,
                that were trained on the <i>full</i> VAW dataset as
                an oracle comparison, SCoNE and TAP. We choose the presently
                popular state-of-the-art methods,
                including OVR-RCNN, ViLD, Region CLIP,
                PromptDet, and Detic, to participate in the comparison
                for open-vocabulary category prediction. Our best model achieves
                68.52/67.62 AP across all attribute classes for the box-given
                and box-free settings respectively. For open-vocabulary detection,
                our best model obtains 54.10/35.17 AP for novel categories,
                surpassing the recent state-of-the-art ViLD-ens and Detic by a large margin.
            </p>
            <div style="text-align: center;">
                <img src="resources/benchmark_on_coco_vaw.png" width="600px">
            </div>
        </td>
    </tr>

    <tr>
        <td>
            <br>
            <p>
                <b>
                    R2: Benchmark on LSA Dataset
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                Pham <i>et al.</i> proposed OpenTAP with a Large-Scale
                object Attribute dataset (LSA). LSA aggregates all images,
                their parsed objects and attributes of
                the Visual Genome (VG), GQA, COCO-Attributes,
                Flickr30K-Entities, MS-COCO, and a portion of Localized Narratives (LNar).
                In the paper, OpenTAP studies the generalizability
                to unseen attributes in two ways:
                LSA common (4921 common attributes for the base set,
                605 common attributes for the novel set);
                LSA common &rightarrow; rare (5526 common attributes for
                the base set, 4012 rare attributes for the novel set).
                As OpenTAP employs a Transformer-based architecture with
                object category and object bounding box as the additional
                prior inputs, we have evaluated a 2-fold architecture
                setting in the proposed OvarNet. One is the original OvarNet
                without any additional input information;
                the other integrates the object category embedding
                as an extra token into the transformer encoder
                layer. The following Tab. shows the comparison results
                with other methods. It indicates that OvarNet outperforms
                prompt based CLIP by a large margin and surpasses OpenTAP
                with additional category embedding.
            </p>
            <div style="text-align: center;">
                <img src="resources/benchmark_on_lsa.png" width="500px">
            </div>
        </td>
    </tr>

    <tr>
        <td>
            <br>
            <p>
                <b>
                    R3: Cross-dataset Transfer on OVAD Benchmark
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                We compare with other state-of-the-art methods in OVAD benchmark,
                reported by OVAD.
                OVAD introduces the open-vocabulary attributes detection
                task with a clean and densely annotated attribute evaluation
                benchmark. The benchmark defines 117 attribute classes with
                over 14,300 object instances. Following the same evaluation
                protocol, we conduct zero-shot cross-dataset transfer evaluation
                on OVAD benchmark with CLIP-Attr and OvarNet trained on COCO
                Caption dataset. Metric is average precision (AP) over
                different attribute frequency distribution, 'head', 'medium',
                and 'tail'. The methods for the comparison are OV-Faster-RCNN,
                Detic, OVD, LocOv, OVR, CLIP, OpenCLIP, ALBEF, BLIP,
                X-VLM, and OVAD provided in OVAD benchmark.
                The following Tab. presents that our method outperforms
                others by a large margin.
            </p>
            <div style="text-align: center;">
                <img src="resources/benchmark_on_ovad.png" width="500px">
            </div>
        </td>
    </tr>

</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Visualizations</h2>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                In the following Fig., we show the qualitative results
                of OvarNet on VAW and MS-COCO benchmarks.
                OvarNet is capable of accurately localizing, recognizing,
                and characterizing objects based on a broad variety
                of novel categories and attributes.
            </p>
            <div style="text-align: center;">
                <img src="resources/qualitative_results.png" width="800px">
            </div>
        </td>
    </tr>
</table>

<br>
<br>

</body>
</html>
