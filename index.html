<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->
    <style type="text/css">
        @font-face {
            font-family: 'Avenir Book';
            src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
        }
    body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
	<title>Building Extraction from Remote Sensing Images with Sparse Token Transformers</title>
</head>

<body>
<br>
<span style="font-size:36px">
    <div style="text-align: center;">
        Building Extraction from Remote Sensing Images with Sparse Token Transformers
    </div>
</span>
<br>
<br>
<br>
<table align="center" width="800px">
    <tr>
        <td align="center" width="180px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://kyanchen.github.io/">Keyan Chen</a><sup>1,2,3</sup></span>
            </div>
        </td>
        <td align="center" width="180px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://zhengxiazou.github.io/">Zhengxia Zou</a><sup>1,2,3</sup></span>
            </div>
        </td>

        <td align="center" width="140px">
            <div style="text-align: center;">
                <span style="font-size:16px">
                    <a href="http://levir.buaa.edu.cn/">Zhenwei Shi</a>
<!--                    <sup><img class="round" style="width:20px" src="./resources/corresponding_fig.png">3</sup>-->
                    <sup>&#9993 1,2,3</sup>
                </span>
            </div>
        </td>
    </tr>
</table>

<br>
	
<table align="center" width="800px">
    <tbody>
    <tr>
        <td align="center" width="200px">
            <center>
                <span style="font-size:16px">Beihang University<sup>1</sup></span>
            </center>
        </td>

        <td align="center" width="200px">
            <center>
                <span style="font-size:16px">Beijing Key Laboratory of Digital Media<sup>2</sup></span>
            </center>
        </td>

        <td align="center" width="200px">
            <center>
                <span style="font-size:16px">State Key Laboratory of Virtual Reality Technology and Systems<sup>3</sup></span>
            </center>
        </td>
    </tr>
    </tbody>
</table>


<!--<table align="center" width="700px">-->
<!--    <tbody>-->
<!--    <tr>-->
<!--        <td align="center" width="300px">-->
<!--            <center>-->
<!--                <span style="font-size:16px"><sup>&star;</sup>equal contribution</span>-->
<!--            </center>-->
<!--        </td>-->

<!--        <td align="center" width="300px">-->
<!--            <center>-->
<!--                <span style="font-size:16px"><sup>&#9993</sup>corresponding author</span>-->
<!--            </center>-->
<!--        </td>-->
<!--    </tr>-->
<!--    </tbody>-->
<!--</table>-->

<table align="center" width="700px">
    <tbody>
    <tr>
        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Code
                    <a href="https://github.com/KyanChen/STT">[GitHub]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Paper
                    <a href="http://levir.buaa.edu.cn/publications/STT_RS_.pdf">[PDF]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <center>
                <span style="font-size:20px">
                    Cite <a href="resources/cite.txt"> [BibTeX]</a>
                </span>
            </center>
        </td>
    </tr>
    </tbody>
</table>
<br>
<hr>

<div style="text-align: center;">
    <h2>Teaser</h2>
</div>


<p style="text-align:justify; text-justify:inter-ideograph;">
<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/sota.png" width="600px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                The Fig. shows the speedâ€“accuracy trade-off and some
                comparable results between the proposed method and
                other state-of-the-art segmentation methods.
                Throughput (images with 512 &times; 512 pixels per
                second on a 2080Ti GPU) versus accuracy (IoU) on
                WHU aerial building extraction test set.
                Here, we only calculate the model inference time,
                not including the time to read images.
                Our model (SST) outperforms other segmentation methods
                with a clear margin. For STT, Base (S4), and Base (S5),
                points on the line from the left to the right refers to the
                models with different CNN feature extractor of ResNet50, VGG16 and
                ResNet18, respectively.
            </p>
        </td>
    </tr>
</table>


<br>
<hr>
<div style="text-align: center;">
    <h2>Abstract</h2>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
            Deep learning methods have achieved considerable progress
                in remote sensing image building extraction.
                Most building extraction methods are based on Convolutional
                Neural Networks (CNN). Recently, vision transformers have provided
                a better perspective for modeling long-range context in images,
                but usually suffer from high computational complexity and memory usage.
                In this paper, we explored the potential of using transformers for efficient
                building extraction. We design an efficient dual-pathway
                transformer structure that learns the long-term dependency
                of tokens in both their spatial and channel dimensions and
                achieves state-of-the-art accuracy on benchmark building
                extraction datasets. Since single buildings in remote sensing
                images usually only occupy a very small part of the image pixels,
                we represent buildings as a set of "sparse" feature vectors
                in their feature space by introducing a new module
                called "sparse token sampler".
                With such a design, the computational complexity
                in transformers can be greatly reduced over an
                order of magnitude. We refer to our method as Sparse
                Token Transformers (STT). Experiments conducted on the
                Wuhan University Aerial Building Dataset (WHU) and the
                Inria Aerial Image Labeling Dataset (INRIA) suggest the
                effectiveness and efficiency of our method.
                Compared with some widely used segmentation methods
                and some state-of-the-art building extraction methods,
                STT has achieved the best performance with low time cost.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Architecture</h2>
</div>

<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/model.png" width="800px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <br>
                An overview of the proposed method.
                Our method consists of a CNN feature extractor,
                a spatial/channel sparse token sampler,
                a transformer-based encoder/decoder,
                and a prediction head.
                In the transformer part,
                when modeling the long-range dependency between different
                channels/spatial locations of the input image,
                instead of using all features vectors,
                we select the most important <i>k</i><sub>s</sub> (<i>k</i><sub>s</sub> &#x226A; <i>HW</i>)
                spatial tokens and <i>k</i><sub>c</sub> (<i>k</i><sub>c</sub> &#x226A; <i>C</i>) channel tokens.
                The sparse tokens greatly reduce the computational and memory consumption.
                Based on the semantic tokens generated by the encoder,
                a transformer decoder is employed to refine the original features.
                Finally, in our prediction head,
                we apply two upsampling layers to
                produce high-resolution building extraction results.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Quantitative Results</h2>
</div>

<table>
    <tr>
        <td>
            <p>
                <b>
                    R1: Benchmark on WHU and INRIA building datasets
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                Comparison with some well-known image
                labeling methods and state-of-the-art building extraction
                methods on the WHU and INRIA building datasets.
                UNet, SegNet, DANet, and DeepLabV3 are commonly
                used methods for segmentation tasks in CNN framework.
                SETR is a transformer-based method for segmentation.
                The methods mentioned in the second row are all based on
                the CNN framework for specific building extraction tasks.
                To validate the efficiency, we report number of parameters
                (<a>Params.</a>), multiply-accumulate operations (<a>MACs</a>)
                and images with 512 &times; 512 pixels
                per second on a 2080Ti GPU (<a>Throughput</a>).
            </p>
            <div style="text-align: center;">
                <img src="resources/benchmark_on_INRIA_WHU.png" width="700px">
            </div>
        </td>
    </tr>

</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Visualizations</h2>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                In the following Fig., we show the qualitative results
                of STT on WHU and INRIA benchmarks.
                The results of different methods
                on samples from the WHU (a-e)
                and INRIA (f-j) building datasets are visualized.
                The figure is colored differently to facilitate viewing,
                with <span style="color: black "><b>white</b></span> representing true positive pixels,
                <span style="color: black "><b>black</b></span> representing true negative pixels,
                <span style="color: red "><b>red</b></span> representing false positive pixels,
                and <span style="color: green "><b>green</b></span> representing false negative pixels.
            </p>
            <div style="text-align: center;">
                <img src="resources/comparison_merhods_WHU.png" width="800px">
                <img src="resources/comparison_merhods_INRIA.png" width="800px">
            </div>
        </td>
    </tr>
</table>

<br>
<br>

</body>
</html>
